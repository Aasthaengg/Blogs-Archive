# Blogs-Archive
## Here is the link to all my blogs:

<ol>
  <li>
    <A href="https://medium.com/geekculture/auto-code-generation-using-gpt-2-4e81cb05430?source=---------0----------------------------">Auto-code generation using GPT-2</a>
    <br>
    <br>
    <center>
      <img src="https://miro.medium.com/max/682/1*BQ5pUlbr_wYOrzVSdPbOTQ.jpeg">
    </center>
    <p>GPT-2 stands for “Generative Predictive Transformer”. It is an open-source model trained on an over 1.5 Billion parameters for generating the next sequence of text, for a give sequence. The GPT-2 has a remarkable and amazing competence to generate texts, much beyond the expectations of conventional language models. Thi blog outlines the task of auto code-generation using GPT-2.</p>
     <ul> Blog Contains:
      <li>About GPT-2 and why its advanced version is termed  "dangerous".</li>
      <li>GPT-2 based concepts of the transformers.</li>
      <li>Working mechanism of GPT-2.</li>
      <li>About auto code generation.</li>
      <li>The steps to fine-tune GPT-2 for code generation.</li>
      <li>How this use case is helping tech professionals?</li>
      <li>How RoBERTa is different from BERT</li>
      <li>My perespective on GPT architecture.</li>
      
   </ul>
    
  </li>
  
  <li>
    <A href="https://aastha-eng.medium.com/evolving-with-bert-introduction-to-roberta-5174ec0e7c82">Evolving with BERT: Introduction to RoBERTa</a>
    <br>
    <br>
    <center>
      <img src="https://miro.medium.com/max/2000/1*yZ7gFAzfRFa3Cua87DJE7w.gif">
    </center>
    <p>RoBERTa is an extension of BERT with changes to the pretraining procedure. The modifications include: training the model longer, with bigger batches, over more data removing the next sentence prediction objective training on longer sequences dynamically changing the masking pattern applied to the training data.Here is a blog authored by me where I have discussed about the training strategie used to modify BERT.</p>
In this blog I have discussed how the Facebook Research AI agency modified the training procedure of the existing Google BERT, proving to the world that there is always room to improve. The topics covered in the blog are: 
    <ul>
      <li>Introduction to Open Source BERT by Google.</li>
      <li>Transformer model — a foundational concept for BERT. </li>
      <li>Training of BERT.</li>
      <li>Optimization of BERT: It includes atering the training procedures of BERT.</li>
      <li>Introduction to Facebook’s RoBERTa as an optimized method for pretraining self-supervised             NLP systems. </li>
      <li>Why RoBERTa matters?</li>
      <li>How RoBERTa is different from BERT</li>
      <li>Zero-Shot Learning with RoBERTa</li>
      <li>Roberta For Sequence Classification.</li>
    </ul>
  </li>
  
  <li>
    <A href="https://medium.com/nerd-for-tech/key-feature-extraction-from-classified-summary-of-a-text-file-using-bert-c1472f7b493?source=---------3----------------------------">Key Feature extraction from classified summary of a Text file using BERT</a>
    <br>
    <br>
    <center>
      <img src="https://miro.medium.com/max/768/0*4dWSd7wQgBTPQegM.png">
    </center>
    <p>It is an end to end Machine Learning project helpful for retail and ecomercial usage. 
In this blog, I have showed how BERT solves a basic text summarization and categorization issue.</p>
    <ul>Blog Contains:
      <li>Structure of BERT as summarizer and classifier.</li>
      <li>Comparing BERT with XLNet & GPT-2, for Text Summarization based on performance.</li>
      <li>Reason for choosing the BERT Model and selecting its version DistilBERT. </li>
      <li>Full fledged notebook with Tensorflow codes depicting  implementation of Text                         classification using BERT </li>
      <li>Explaing extractive, abstractive, and mixed summarization strategies.</li>
      <li>Testing the model by passing Input to the trained model to summarize and then classify the text.</li>
      <li>Key Feature Extraction of obtained text using spaCyNER.</li>
      <li>Embracing Further Thoughts of the readers.</li>
    </ul>
  </li>
  
  
  <li>
    <A href="https://medium.com/nerd-for-tech/yolo-you-only-look-once-65ea86104c51?source=---------4----------------------------">YOLO: You Only Look Once</a>
    <br>
    <br>
    <center>
      <img src="https://miro.medium.com/max/875/0*pP8_Um1rRy1nAQUS.jpg">
    </center>
    <p>YOLO is a deep learning based approach of object detection. In this blog I have used a custome dataset namely Blood Cell Object Detection dataset for training YOLOv5 on PyTorch.</p>
  </li>
  
  
  <li>
    <A href="https://medium.com/nerd-for-tech/introduction-to-numpy-68e3f0ee7206?source=---------2----------------------------">Introduction to NumPy</a>
    <br>
    <br>
    <center>
      <img src="https://miro.medium.com/max/875/1*IjtlGvbMeT89fQdYXwKk2g.png">
    </center>
    <p>In this blog I take the readers on a journey first  to a world without NumPy and then to the impact of it's existence.</p>
    <ul>Blog Contains:
      <li>Core Python vs NumPy with Python.</li>
      <li>Built-in array vs NumPy array.</li>
      <li>Python list vs Python array.</li>
      <li>Python list vs NumPy array.</li>
      <li>Discussing Speed of NumPy and why does it matter.</li>
      <li>How to utilise NumPy to its best potential by exhibiting how to use Vectorization and Broadcasting.</li>
     
  </ul>
  </li>
  
  
