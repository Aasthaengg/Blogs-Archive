# Blogs-Archive
## Here is the link to all my blogs:

<ol>
  <li>
    <A href="https://medium.com/geekculture/auto-code-generation-using-gpt-2-4e81cb05430?source=---------0----------------------------">Auto-code generation using GPT-2</a>
    <br>
    <br>
    <center>
      <img src="https://miro.medium.com/max/682/1*BQ5pUlbr_wYOrzVSdPbOTQ.jpeg">
    </center>
    <p>GPT-2 stands for “Generative Predictive Transformer”. It is an open-source model trained on an over 1.5 Billion parameters for generating the next sequence of text, for a give sequence. The GPT-2 has a remarkable and amazing competence to generate texts, much beyond the expectations of conventional language models. Thi blog outlines the task of auto code-generation using GPT-2.</p>
  </li>
  
  <li>
    <A href="https://aastha-eng.medium.com/evolving-with-bert-introduction-to-roberta-5174ec0e7c82">Evolving with BERT: Introduction to RoBERTa</a>
    <br>
    <br>
    <center>
      <img src="https://miro.medium.com/max/2000/1*yZ7gFAzfRFa3Cua87DJE7w.gif">
    </center>
    <p>RoBERTa is an extension of BERT with changes to the pretraining procedure. The modifications include: training the model longer, with bigger batches, over more data removing the next sentence prediction objective training on longer sequences dynamically changing the masking pattern applied to the training data.Here is a blog authored by me where I have discussed about the training strategie used to modify BERT.</p>
  </li>
  
  <li>
    <A href="https://medium.com/nerd-for-tech/key-feature-extraction-from-classified-summary-of-a-text-file-using-bert-c1472f7b493?source=---------3----------------------------">Key Feature extraction from classified summary of a Text file using BERT</a>
    <br>
    <br>
    <center>
      <img src="https://miro.medium.com/max/768/0*4dWSd7wQgBTPQegM.png">
    </center>
    <p>BERT, in a nutshell, is a model that understands how to represent text. You feed it a sequence, and it scans left and right a number of times before producing a vector representation for each word as an output. BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). In this post, I’ll show you how BERT solves a basic text summarization and categorization issue.</p>
  </li>
  
  
  <li>
    <A href="https://medium.com/nerd-for-tech/yolo-you-only-look-once-65ea86104c51?source=---------4----------------------------">YOLO: You Only Look Once</a>
    <br>
    <br>
    <center>
      <img src="https://miro.medium.com/max/875/0*pP8_Um1rRy1nAQUS.jpg">
    </center>
    <p>YOLO is a deep learning based approach of object detection. In this blog I have used a custome dataset namely Blood Cell Object Detection dataset for training YOLOv5 on PyTorch.</p>
  </li>
  
  
